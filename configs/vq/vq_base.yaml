# @package _global_
models:
  perceptual_grouping:
    pool:
      _target_: ocl.slot_dict.VectorQuantize
      dim: ${models.conditioning.object_dim}
      codebook_size: 256
      decay: 0.99
      heads: 1
      separate_codebook_per_head: false
      kmeans_init: true
      commitment_weight: 0.1
      affine_param: false
      straight_through: false
      stochastic_sample_codes: false
      codebook_dim: ${models.conditioning.object_dim}
      threshold_ema_dead_code: 0.8 # 1*(0.99)^25 = 0.77, if some code have not selected roughly 20 iterations, it is replaced with random embedding from the batch
      use_cosine_sim: false