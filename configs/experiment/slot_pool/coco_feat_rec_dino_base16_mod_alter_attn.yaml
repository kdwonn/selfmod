# @package _global_
defaults:
  - /experiment/slot_pool/_base_abs_decoding
  - /dataset: coco
  - /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop
  - /experiment/slot_pool/_metrics_coco_abs_decoding
  - optional /vq: vq
  - _self_

# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.
logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  notes: null
  tags: null
  offline: false
  group: ${hydra:runtime.choices.dataset}
  project: ocl-pool

visualizations:
  pool_indices:
    _target_: routed.ocl.visualizations.IndexHistogram
    pool_size: ${models.perceptual_grouping.pool.codebook_size}
    indices_path: perceptual_grouping.pool_indices
  captioned_attn:
    _target_: routed.ocl.visualizations.IndexCaptionedImage
    image_vis:
      _target_: ocl.visualizations.ResizedMask
      target_size: 64
      patch_mode: true
    image_path: perceptual_grouping.feature_attributions
    indices_path: perceptual_grouping.pool_indices
  pre_attn:
    _target_: routed.ocl.visualizations.ResizedMask
    target_size: 64
    patch_mode: true
    mask_path: perceptual_grouping.pre_attn
  # pre_pred_segmentation:
  #   _target_: routed.ocl.visualizations.Segmentation
  #   denormalization:
  #     _target_: ocl.preprocessing.Denormalize
  #     mean: [0.485, 0.456, 0.406]
  #     std: [0.229, 0.224, 0.225]
  #   image_path: input.image
  #   mask_path: perceptual_grouping.pre_object_decoder.masks_as_image

training_metrics:
  perplexity:
    _target_: routed.ocl.metrics.pool.Perplexity
    pool_size: ${models.perceptual_grouping.pool.codebook_size}
    base: 2
    indices_path: perceptual_grouping.pool_indices
  unique_ratio:
    _target_: routed.ocl.metrics.UniqueRatio
    slot_num: ${models.conditioning.n_slots}
    indices_path: perceptual_grouping.pool_indices
  pool_usage:
    _target_: routed.ocl.metrics.CodebookUsage
    pool_size: ${models.perceptual_grouping.pool.codebook_size}
    indices_path: perceptual_grouping.pool_indices
  l2_dist_pre_post_slots:
    _target_: routed.ocl.metrics.TensorDistStatistic
    dist: l2
    tensor1_path: perceptual_grouping.objects
    tensor2_path: perceptual_grouping.pre_slots
  cos_dist_pre_post_slots:
    _target_: routed.ocl.metrics.TensorDistStatistic
    dist: cos
    tensor1_path: perceptual_grouping.objects
    tensor2_path: perceptual_grouping.pre_slots
  logit_entropy:
    _target_: routed.ocl.metrics.pool.Entropy
    preds_path: perceptual_grouping.pool_logit

training_vis_frequency: 10000

trainer:
  devices: 1
  max_steps: 500000
  max_epochs:
  check_val_every_n_epoch: null
  val_check_interval: 5000
  gradient_clip_val: 1.0
  callbacks:
    - _target_: ocl.callbacks.GumbelTempAnnealing
      initial_temp: ${models.perceptual_grouping.gumbel_temperature}
      starting_iter: 10000
      iter_to_anneal: 50000
      min_temp: 0.2

dataset:
  num_workers: 4
  batch_size: 64

experiment:
  input_feature_dim: 768

alter_opt: attn
alter_interval: 3
alter_warmup_iter: 10000

models:
  conditioning:
    _target_: routed.ocl.conditioning.RandomConditioning
    n_slots: 7
    object_dim: 256
    batch_size_path: input.batch_size
  feature_extractor:
    model_name: vit_base_patch16_224_dino
    pretrained: ${when_testing:false,true}
    freeze: true
  perceptual_grouping:
    _target_: routed.ocl.perceptual_grouping_modular.SlotAttentionGroupingModAltAttn
    reduction: false
    rank: 8
    gumbel_temperature: 2
    ff_mlp:
      _target_: ocl.neural_networks.build_two_layer_mlp
      input_dim: ${..object_dim}
      output_dim: ${..object_dim}
      hidden_dim: "${eval_lambda:'lambda dim: 4 * dim', ${..object_dim}}"
      initial_layer_norm: false
      residual: false
    object_decoder:
      _target_: ocl.decoding.AutoregressivePatchDecoder
      decoder_cond_dim: ${.output_dim}
      use_input_transform: true
      use_decoder_masks: true
      decoder:
        _target_: ocl.neural_networks.build_transformer_decoder
        _partial_: true
        n_layers: 4
        n_heads: 8
        return_attention_weights: true
    train_base_path: input.train_base
  
losses:
  commit:
    _target_: routed.ocl.losses.ScalarWrapperLoss
    weight: 0
    loss_path: perceptual_grouping.commit_loss
  # mse_pre:
  #   _target_: routed.ocl.losses.ReconstructionLoss
  #   loss_type: mse
  #   input_path: perceptual_grouping.pre_object_decoder.reconstruction
  #   target_path: perceptual_grouping.pre_object_decoder.target
  mse:
    _target_: routed.ocl.losses.ReconstructionLoss
    loss_type: mse
    input_path: perceptual_grouping.dec_to_train.reconstruction
    target_path: perceptual_grouping.dec_to_train.target  # Object decoder does some resizing.

lr_scale: 1

optimizers:
  opt0:
    parameter_groups:
      # Optimize feature_extractor, perceptual_grouping, conditioning with lower learning rate
      - params: [models.feature_extractor, models.conditioning]
      # Apply weight decay to object_decoder, but not to bias parameters
      - params: models.perceptual_grouping
        predicate: "${lambda_fn:'lambda name, param: \"decoder\" in name'}"
      - params: models.perceptual_grouping
        predicate: "${lambda_fn:'lambda name, param: \"positional_embedding\" in name'}"
      - params: models.perceptual_grouping
        predicate: "${lambda_fn:'lambda name, param: \"pool\" in name'}"
      - params: models.perceptual_grouping
        predicate: "${lambda_fn:'lambda name, param: \"adamlp\" not in name and \"lora\" not in name and \"decoder\" not in name and \"positional_embedding\" not in name and \"pool\" not in name'}"
      - params: models.perceptual_grouping
        predicate: "${lambda_fn:'lambda name, param: \"adamlp\" in name or \"lora\" in name'}"
        lr: "${eval_lambda:'lambda x, y: x * y', ${lr_scale}, ${optimizers.opt0.optimizer.lr}}"

# optimizers:
#   opt0:
#     parameter_groups:
#       # Optimize feature_extractor, perceptual_grouping, conditioning with lower learning rate
#       - params: [models.feature_extractor, models.conditioning]
#       # Apply weight decay to object_decoder, but not to bias parameters
#       - params: models.perceptual_grouping
#         predicate: "${lambda_fn:'lambda name, param: \"adamlp\" not in name and \"lora\" not in name'}"
#   opt1:
#     _target_: ocl.optimization.OptimizationWrapper
#     optimizer:
#       _target_: torch.optim.Adam
#       _partial_: true
#       lr: 0.0004
#     parameter_groups:
#       - params: models.perceptual_grouping
#         predicate: "${lambda_fn:'lambda name, param: \"adamlp\" in name or \"lora\" in name'}"
#         lr: "${eval_lambda:'lambda x, y: x * y', ${lr_scale}, ${optimizers.opt0.optimizer.lr}}"
#     lr_scheduler:
#       _target_: ocl.scheduling.exponential_decay_after_optional_warmup
#       _partial_: true
#       decay_rate: 0.5
#       decay_steps: 100000
#       warmup_steps: 0

evaluation_metrics:
  pre_instance_mask_ari:
    _target_: routed.ocl.metrics.ARIMetric
    prediction_path: perceptual_grouping.pre_object_decoder.masks_as_image
    target_path: input.instance_mask
    foreground: false
    convert_target_one_hot: true
    ignore_overlaps: true
  pre_instance_abo:
    _target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric
    prediction_path: perceptual_grouping.pre_object_decoder.masks_as_image
    target_path: input.instance_mask
    use_threshold: false
    matching: best_overlap
    ignore_overlaps: true
  pre_segmentation_abo:
    _target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric
    prediction_path: perceptual_grouping.pre_object_decoder.masks_as_image
    target_path: input.segmentation_mask
    use_threshold: false
    matching: best_overlap
    ignore_overlaps: true
  perplexity:
    _target_: routed.ocl.metrics.pool.Perplexity
    pool_size: ${models.perceptual_grouping.pool.codebook_size}
    base: 2
    indices_path: perceptual_grouping.pool_indices
  pool_usage:
    _target_: routed.ocl.metrics.CodebookUsage
    pool_size: ${models.perceptual_grouping.pool.codebook_size}
    indices_path: perceptual_grouping.pool_indices
