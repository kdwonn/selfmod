# @package _global_
defaults:
  - /experiment/slot_pool/_base_abs_decoding
  - /dataset: coco
  - /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop
  - /experiment/slot_pool/_metrics_coco_abs_decoding
  - optional /vq: vq
  - _self_

# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.
logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  notes: null
  tags: null
  offline: false
  group: ${hydra:runtime.choices.dataset}
  project: ocl-pool

visualizations:
  pool_indices:
    _target_: routed.ocl.visualizations.IndexHistogram
    pool_size: ${models.perceptual_grouping.pool.codebook_size}
    indices_path: perceptual_grouping.pool_indices
  TD_signal:
    _target_: routed.ocl.visualizations.ResizedMask
    target_size: 92
    patch_mode: true
    mask_path: perceptual_grouping.alpha_weights
  masked_image:
    _target_: routed.ocl.visualizations.MaskedImage
    denormalization:
      _target_: ocl.preprocessing.Denormalize
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
    n_objects: ${models.conditioning.n_slots}
    mask_path: perceptual_grouping.object_decoder.masks_as_image
    image_path: input.image
  captioned_TD:
    _target_: routed.ocl.visualizations.IndexCaptionedImage
    image_vis:
      _target_: ocl.visualizations.ResizedMask
      target_size: 92
      patch_mode: true
    image_path: perceptual_grouping.alpha_weights
    indices_path: perceptual_grouping.pool_indices

training_metrics:
  perplexity:
    _target_: routed.ocl.metrics.pool.Perplexity
    pool_size: ${models.perceptual_grouping.pool.codebook_size}
    base: 2
    indices_path: perceptual_grouping.pool_indices
  unique_ratio:
    _target_: routed.ocl.metrics.UniqueRatio
    slot_num: ${models.conditioning.n_slots}
    indices_path: perceptual_grouping.pool_indices
  pool_usage:
    _target_: routed.ocl.metrics.CodebookUsage
    pool_size: ${models.perceptual_grouping.pool.codebook_size}
    indices_path: perceptual_grouping.pool_indices
  norm_feat_to_feedback:
    _target_: routed.ocl.metrics.TensorNormRatioStatistic
    dim: -1
    tensor1_path: perceptual_grouping.pos_feat
    tensor2_path: perceptual_grouping.feedback
  norm_feat:
    _target_: routed.ocl.metrics.TensorNormStatistic
    dim: -1
    tensor_path: perceptual_grouping.pos_feat
  norm_feedback:
    _target_: routed.ocl.metrics.TensorNormStatistic
    dim: -1
    tensor_path: perceptual_grouping.feedback
  norm_k_inputs:
    _target_: routed.ocl.metrics.TensorNormStatistic
    dim: -1
    tensor_path: perceptual_grouping.k_inputs
  norm_enc_feat:
    _target_: routed.ocl.metrics.TensorNormStatistic
    dim: -1
    tensor_path: perceptual_grouping.object_decoder.target

training_vis_frequency: 10000

trainer:
  devices: 1
  max_steps: 500000
  max_epochs:
  check_val_every_n_epoch: null
  val_check_interval: 5000

dataset:
  num_workers: 4
  batch_size: 64

experiment:
  input_feature_dim: 768


models:
  conditioning:
    _target_: routed.ocl.conditioning.RandomConditioning
    n_slots: 7
    object_dim: 256

    batch_size_path: input.batch_size
  feature_extractor:
    model_name: vit_base_patch16_224_dino
    pretrained: ${when_testing:false,true}
    freeze: true
  perceptual_grouping:
    _target_: routed.ocl.perceptual_grouping.SlotAttentionGroupingABSDecodingIterQuant
    feat_num: 196
    feedback_type: v
    synth_feat_without_pos: true
    num_blocks: 1
    object_decoder:
      _target_: ocl.decoding.PatchDecoder
      decoder:
        _target_: ocl.neural_networks.build_mlp
        _partial_: true
        features: [2048, 2048, 2048]
  
losses:
  commit:
    _target_: routed.ocl.losses.ScalarWrapperLoss
    weight: 1.0
    loss_path: perceptual_grouping.commit_loss
