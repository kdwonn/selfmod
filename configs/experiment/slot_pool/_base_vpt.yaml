# @package _global_
# Default parameters for slot attention with a ViT decoder for feature reconstruction.

defaults:
  - /experiment/_output_path
  - /training_config
  - _self_

trainer:
  gradient_clip_val: 1.0

experiment:
  input_feature_dim: 384

models:
  feedback_model:
    _target_: routed.ocl.feedback_model.FeedbackVPT
    feedback_prob: 0.5

    feature_extractor:
      _target_: ocl.feature_extractors.TimmFeatureExtractorPrompt
      model_name: vit_small_patch16_224_dino
      pretrained: false
      freeze: true
      feature_level: 12

    conditioning:

    perceptual_grouping:
      _target_: ocl.perceptual_grouping.SlotAttentionGrouping
      feature_dim: ${.object_dim}
      object_dim: ${models.feedback_model.conditioning.object_dim}
      use_projection_bias: false
      positional_embedding:
        _target_: ocl.neural_networks.wrappers.Sequential
        _args_:
          - _target_: ocl.neural_networks.positional_embedding.DummyPositionEmbed
          - _target_: ocl.neural_networks.build_two_layer_mlp
            input_dim: ${experiment.input_feature_dim}
            output_dim: ${....feature_dim}
            hidden_dim: ${experiment.input_feature_dim}
            initial_layer_norm: true
      ff_mlp:
        _target_: ocl.neural_networks.build_two_layer_mlp
        input_dim: ${..object_dim}
        output_dim: ${..object_dim}
        hidden_dim: "${eval_lambda:'lambda dim: 4 * dim', ${..object_dim}}"
        initial_layer_norm: true
        residual: true
        
    object_decoder:
      object_dim: ${models.feedback_model.perceptual_grouping.object_dim}
      output_dim: ${experiment.input_feature_dim}
      num_patches: 196
    
    image_path: input.image
    batch_size_path: input.batch_size

losses:
  mse:
    _target_: routed.ocl.losses.ReconstructionLoss
    loss_type: mse
    input_path: feedback_model.object_decoder.reconstruction
    target_path: feedback_model.object_decoder.target  # Object decoder does some resizing.

visualizations:
  input:
    _target_: routed.ocl.visualizations.Image
    denormalization:
      _target_: ocl.preprocessing.Denormalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
    image_path: input.image
  masks:
    _target_: routed.ocl.visualizations.Mask
    mask_path: feedback_model.object_decoder.masks_as_image
  pred_segmentation:
    _target_: routed.ocl.visualizations.Segmentation
    denormalization:
      _target_: ocl.preprocessing.Denormalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
    image_path: input.image
    mask_path: feedback_model.object_decoder.masks_as_image
optimizers:
  opt0:
    _target_: ocl.optimization.OptimizationWrapper
    optimizer:
      _target_: torch.optim.Adam
      _partial_: true
      lr: 0.0004
    lr_scheduler:
      _target_: ocl.scheduling.exponential_decay_after_optional_warmup
      _partial_: true
      decay_rate: 0.5
      decay_steps: 100000
      warmup_steps: 10000
