# @package _global_
# Evaluate center crop masks on COCO.

defaults:
  - /evaluation_config
  - /evaluation/projects/bridging/_base_metrics
  - /evaluation/projects/bridging/_preprocessing_coco
  - /evaluation/projects/bridging/_metrics_discovery_masks
  - /evaluation/projects/bridging/_metrics_segmentation
  - /dataset: coco
  - _self_

eval_batch_size: 16

modules:
  maskcut:
    _target_: routed.ocl.maskcut.MaskCut
    feature_path: feature_extractor.features

train_config_overrides:
  - models.feature_extractor.feature_level=key12

load_checkpoint: False

# save_outputs: true
# skip_metrics: true
# n_samples_to_store: 30
# outputs_to_store:
#   - input.orig_image
#   - input.instance_mask
#   - maskcut.masks


evaluation_metrics:
  instance_mask_ari:
    prediction_path: maskcut.masks
  instance_mask_unsup_iou:
    prediction_path: maskcut.masks
  instance_mask_abo:
    prediction_path: maskcut.masks
  instance_mask_corloc:
    prediction_path: maskcut.masks
  instance_mask_recall:
    prediction_path: maskcut.masks
  segmentation_mask_ari:
    prediction_path: maskcut.masks
  segmentation_mask_unsup_iou:
    prediction_path: maskcut.masks
  segmentation_mask_abo:
    prediction_path: maskcut.masks

#  for output save

# dataset:
#   eval_transforms:
#     02a_preprocessing:
#       _target_: ocl.transforms.Map
#       transform: "${lambda_fn:'lambda data: {\"orig_image\": data[\"image\"], **data}'}"
#       fields:
#         - image
#       batch_transform: false
#     03b_preprocessing:
#       transforms:
#         orig_image:
#           _target_: torchvision.transforms.Compose
#           transforms:
#             - _target_: torchvision.transforms.ToTensor
#             - _target_: ocl.preprocessing.ResizeNearestExact
#               size: 320
#             - _target_: torchvision.transforms.CenterCrop
#               size: 320

#         image:
#           _target_: torchvision.transforms.Compose
#           transforms:
#             - _target_: torchvision.transforms.ToTensor
#             - _target_: torchvision.transforms.Resize
#               size: 224
#               interpolation: ${torchvision_interpolation_mode:BICUBIC}
#             - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"
#             - _target_: torchvision.transforms.CenterCrop
#               size: 224
#             - _target_: torchvision.transforms.Normalize
#               mean: [0.485, 0.456, 0.406]
#               std: [0.229, 0.224, 0.225]

#         instance_mask:
#           _target_: torchvision.transforms.Compose
#           transforms:
#             - _target_: ocl.preprocessing.DenseMaskToTensor
#             - _target_: ocl.preprocessing.ResizeNearestExact
#               size: 320
#             - _target_: torchvision.transforms.CenterCrop
#               size: 320

#         segmentation_mask:
#           _target_: torchvision.transforms.Compose
#           transforms:
#             - _target_: ocl.preprocessing.DenseMaskToTensor
#             - _target_: ocl.preprocessing.ResizeNearestExact
#               size: 320
#             - _target_: torchvision.transforms.CenterCrop
#               size: 320

#  for eval
dataset:
  eval_transforms:
    03b_preprocessing:
      transforms:
        image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: torchvision.transforms.Resize
              size: 224
              interpolation: ${torchvision_interpolation_mode:BICUBIC}
            - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"
            - _target_: torchvision.transforms.CenterCrop
              size: 224
            - _target_: torchvision.transforms.Normalize
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
        instance_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 320
            - _target_: torchvision.transforms.CenterCrop
              size: 320
        segmentation_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 320
            - _target_: torchvision.transforms.CenterCrop
              size: 320
