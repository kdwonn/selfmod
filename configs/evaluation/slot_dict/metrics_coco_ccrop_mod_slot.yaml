# @package _global_
# Evaluate center crop masks on COCO.

defaults:
  - /evaluation_config
  - /evaluation/projects/bridging/_base_metrics
  - /evaluation/projects/bridging/_preprocessing_coco
  - /evaluation/projects/bridging/_metrics_discovery_masks
  - /evaluation/projects/bridging/_metrics_segmentation
  # - /evaluation/slot_dict/_metrics_absd
  - /dataset: coco
  - _self_

eval_batch_size: 64

evaluation_metrics:
  pre_instance_mask_ari:
    _target_: routed.ocl.metrics.ARIMetric
    prediction_path: masks_resized_pre
    target_path: input.instance_mask
    foreground: false
    convert_target_one_hot: true
    ignore_overlaps: true
  pre_instance_mask_unsup_iou:
    _target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric
    prediction_path: masks_resized_pre
    target_path: input.instance_mask
    ignore_overlaps: true
  pre_instance_mask_abo:
    _target_: routed.ocl.metrics.AverageBestOverlapMetric
    prediction_path: masks_resized_pre
    target_path: input.instance_mask
    ignore_overlaps: true
  pre_instance_mask_corloc:
    _target_: routed.ocl.metrics.MaskCorLocMetric
    prediction_path: masks_resized_pre
    target_path: input.instance_mask
    ignore_overlaps: true
  pre_instance_mask_recall:
    _target_: routed.ocl.metrics.BestOverlapObjectRecoveryMetric
    prediction_path: masks_resized_pre
    target_path: input.instance_mask
    ignore_overlaps: true

modules:
  masks_resized:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: perceptual_grouping.feature_attributions
    size_tensor_path: input.segmentation_mask
    patch_mode: true
    channels_last: false
  masks_resized_pre:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: perceptual_grouping.pre_attn
    size_tensor_path: input.segmentation_mask
    patch_mode: true
    channels_last: false

dataset:
  eval_transforms:
    03b_preprocessing:
      transforms:
        image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: torchvision.transforms.Resize
              size: 224
              interpolation: ${torchvision_interpolation_mode:BICUBIC}
            - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"
            - _target_: torchvision.transforms.CenterCrop
              size: 224
            - _target_: torchvision.transforms.Normalize
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
        instance_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 320
            - _target_: torchvision.transforms.CenterCrop
              size: 320
        segmentation_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 320
            - _target_: torchvision.transforms.CenterCrop
              size: 320
