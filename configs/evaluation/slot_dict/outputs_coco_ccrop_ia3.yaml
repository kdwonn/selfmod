# @package _global_
# Save model predictions on COCO
# dataset:
#   eval_transforms:
#     02a_preprocessing:
#       _target_: ocl.transforms.Map
#       transform: "${lambda_fn:'lambda data: {\"orig_image\": data[\"image\"], **data}'}"
#       fields:
#         - image
#       batch_transform: false
defaults:
  - /evaluation_config
  - /evaluation/projects/bridging/_base_metrics_vpt
  - /evaluation/projects/bridging/_preprocessing_coco
  - /dataset: coco
  - _self_

eval_batch_size: 1

save_outputs: true
skip_metrics: true
n_samples_to_store: 500
outputs_to_store:
  - input.orig_image
  - input.instance_mask
  - masks_resized
  - feedback_model.perceptual_grouping.feature_attributions
  - feedback_model.post_perceptual_grouping.feature_attributions

dataset:
  eval_transforms:
    02a_preprocessing:
      _target_: ocl.transforms.Map
      transform: "${lambda_fn:'lambda data: {\"orig_image\": data[\"image\"], **data}'}"
      fields:
        - image
      batch_transform: false
    03b_preprocessing:
      transforms:
        orig_image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 320
            - _target_: torchvision.transforms.CenterCrop
              size: 320

        image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: torchvision.transforms.Resize
              size: 224
              interpolation: ${torchvision_interpolation_mode:BICUBIC}
            - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"
            - _target_: torchvision.transforms.CenterCrop
              size: 224
            - _target_: torchvision.transforms.Normalize
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]

        instance_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 320
            - _target_: torchvision.transforms.CenterCrop
              size: 320

        segmentation_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 320
            - _target_: torchvision.transforms.CenterCrop
              size: 320
